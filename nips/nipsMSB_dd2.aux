\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{Manifold}
\citation{Maggioni}
\citation{mixtureexperts}
\citation{jiang1999}
\citation{fan1996}
\citation{holmes2010}
\citation{fu2011}
\citation{nott2012}
\citation{tran2012}
\citation{norets2012}
\citation{griffin06}
\citation{dunson2007}
\citation{chung2009}
\citation{tokdar2010}
\citation{tran2012}
\citation{tran2012}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{SparseMoF}
\citation{metis}
\@writefile{toc}{\contentsline {section}{\numberline {2}Setting}{2}{section.2}}
\newlabel{sec:setting}{{2}{2}{Setting\relax }{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Goal}{2}{section.3}}
\newlabel{sec:goal}{{3}{2}{Goal\relax }{section.3}{}}
\citation{Maggioni}
\citation{ChenMaggioni12}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of our generative model and algorithm on a swissroll. The top left panel shows the manifold $\mathcal  {M}$ (a swissroll) embedded in a $p$-dimensional ambient space, where the color indicates the coordinate along the manifold, $\eta $ (only the first 3 dimensions are shown for visualization purposes). The bottom left panel shows the distribution of $Y$ as a function of $\eta $, in particular, $F_{Y|\eta }=\mathcal  {N}({\eta },{\eta }+1)$. The middle and right panels show our estimates of $F_{Y|{\eta }}$ at scales 3 and 4, respectively, which follow from partitioning our data. Sample size was $n=10,000$.}}{3}{figure.1}}
\newlabel{fig:swiss}{{1}{3}{Illustration of our generative model and algorithm on a swissroll. The top left panel shows the manifold $\mc {M}$ (a swissroll) embedded in a $p$-dimensional ambient space, where the color indicates the coordinate along the manifold, $\eta $ (only the first 3 dimensions are shown for visualization purposes). The bottom left panel shows the distribution of $Y$ as a function of $\eta $, in particular, $F_{Y|\eta }=\mc {N}({\eta },{\eta }+1)$. The middle and right panels show our estimates of $F_{Y|{\eta }}$ at scales 3 and 4, respectively, which follow from partitioning our data. Sample size was $n=10,000$}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology}{3}{section.4}}
\newlabel{sec:method}{{4}{3}{Methodology\relax }{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Ms.\nobreakspace  {}Deeds\nobreakspace  {}Framework}{3}{subsection.4.1}}
\newlabel{sub:method}{{4.1}{3}{\msd ~Framework\relax }{subsection.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Tree Decomposition}{3}{subsection.4.1}}
\newlabel{eq:base}{{1}{3}{Tree Decomposition\relax }{equation.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Embeddings}{3}{equation.4.1}}
\citation{Daubechies1992}
\citation{ChenMaggioni12}
\citation{METIS}
\citation{stickbreaking}
\citation{ChenMaggioni12}
\citation{Maggioni}
\citation{Maggioni}
\citation{ChenMaggioni12}
\@writefile{toc}{\contentsline {paragraph}{Family}{4}{equation.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Specific Choices}{4}{subsection.4.2}}
\newlabel{sub:spec}{{4.2}{4}{Specific Choices\relax }{subsection.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Tree Partition}{4}{subsection.4.2}}
\newlabel{eq:stick}{{2}{4}{Tree Partition\relax }{equation.4.2}{}}
\citation{Chauveau98anautomated}
\@writefile{toc}{\contentsline {paragraph}{Embedding}{5}{equation.4.2}}
\@writefile{toc}{\contentsline {paragraph}{Family}{5}{equation.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Inference}{5}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Simulation Studies}{5}{subsection.4.4}}
\newlabel{sec:sim}{{4.4}{5}{Simulation Studies\relax }{subsection.4.4}{}}
\citation{Jung2010}
\citation{Arden2010}
\citation{MRCAP11}
\citation{Mori2006}
\citation{Autism}
\citation{cpac}
\citation{Zou2008}
\citation{power}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Neuroscience Applications}{6}{subsection.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Evaluation Criteria}{6}{subsection.4.6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{6}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Illustrative Example}{6}{subsection.5.1}}
\newlabel{sub:ill}{{5.1}{6}{Illustrative Example\relax }{subsection.5.1}{}}
\citation{Breiman2001}
\newlabel{SC@1}{{5.1}{7}{Illustrative Example\relax }{subsection.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustrative example of model (1) suggesting that our posterior estimates of the conditional density are converging as $n$ increases even when $F_{Y|\eta }$ is highly nonlinear and $F_{X|\eta }$ is very high-dimensional. True (red) and estimated (black) density ($50$th percentile: solid line, $2.5$th and $97.5$th percentiles: dashed lines) for two data positions along the manifold (top panels: $\eta \approx -0.9$, bottom panels: $\eta \approx 0.5$) considering different training set sizes.}}{7}{figure.2}}
\newlabel{plotDensity}{{2}{7}{\SC@CAPtext \relax }{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Quantitative Comparisons for Simulated Data}{7}{subsection.5.2}}
\newlabel{sub:sim}{{5.2}{7}{Quantitative Comparisons for Simulated Data\relax }{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Quantitative Comparisons for Neuroscience Applications}{7}{subsection.5.3}}
\newlabel{sub:real}{{5.3}{7}{Quantitative Comparisons for Neuroscience Applications\relax }{subsection.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{7}{section.6}}
\newlabel{sec:disc}{{6}{7}{Discussion\relax }{section.6}{}}
\citation{Maggioni}
\bibstyle{unsrt}
\bibdata{nipsMSB}
\bibcite{Manifold}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Numerical results for various simulation scenarios. Top plots depict the relative mean-squared error of MSB (our approach), versus CART (red), Lasso (black), and PC regression (green) for as a function of ambient dimension of $x$. Bottom plots depict the ratio of CPU time as a function of sample size. The three simulation scenarios are: swissroll (left), linear subspaces (middle), union of linear subspaces (right). MSB outperforms both CART, Lasso, and PC regression in all three scenarios regardless of ambient dimension ($r_{mse}^{\mathcal  {A}}< 1$ for all $p$). MSB compute time is relatively constant as $n$ or $p$ increase, whereas Lasso's compute time increases, thus, as $n$ or $p$ increase, MSB CPU time becomes less than Lasso's. MSB was always significantly faster than CART and PC regression, regardless of $n$ or $p$. For all panels, $n=100$ when $p$ varies, and $p=300$k when $n$ varies, where k indicates $1000$, e.g., $300$k$=3\times 10^5$. }}{8}{figure.3}}
\newlabel{fig:boxplots}{{3}{8}{Numerical results for various simulation scenarios. Top plots depict the relative mean-squared error of MSB (our approach), versus CART (red), Lasso (black), and PC regression (green) for as a function of ambient dimension of $x$. Bottom plots depict the ratio of CPU time as a function of sample size. The three simulation scenarios are: swissroll (left), linear subspaces (middle), union of linear subspaces (right). MSB outperforms both CART, Lasso, and PC regression in all three scenarios regardless of ambient dimension ($r_{mse}^{\mc {A}}< 1$ for all $p$). MSB compute time is relatively constant as $n$ or $p$ increase, whereas Lasso's compute time increases, thus, as $n$ or $p$ increase, MSB CPU time becomes less than Lasso's. MSB was always significantly faster than CART and PC regression, regardless of $n$ or $p$. For all panels, $n=100$ when $p$ varies, and $p=300$k when $n$ varies, where k indicates $1000$, e.g., $300$k$=3\times 10^5$}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Neuroscience application quantitative performance comparisons. Squared error predictive accuracy per subject (using leave-one-out) was computed. We report the mean and standard deviation (s.d.) across subjects of squared error, and CPU time (in seconds). We compare multiscale stick-breaking (MSB), CART, Lasso, random forest (RF), and PC regression. MSB outperforms all the competitors in terms of predictive accuracy and scalability. Only MSB and Lasso even ran for the $\approx 10^6$ dimensional application. \textbf  {Bold} indicates best MSE, $^*$ indicates best CPU time.}}{8}{table.1}}
\newlabel{real}{{1}{8}{Neuroscience application quantitative performance comparisons. Squared error predictive accuracy per subject (using leave-one-out) was computed. We report the mean and standard deviation (s.d.) across subjects of squared error, and CPU time (in seconds). We compare multiscale stick-breaking (MSB), CART, Lasso, random forest (RF), and PC regression. MSB outperforms all the competitors in terms of predictive accuracy and scalability. Only MSB and Lasso even ran for the $\approx 10^6$ dimensional application. \textbf {Bold} indicates best MSE, $^*$ indicates best CPU time}{table.1}{}}
\bibcite{Maggioni}{2}
\bibcite{mixtureexperts}{3}
\bibcite{jiang1999}{4}
\bibcite{fan1996}{5}
\bibcite{holmes2010}{6}
\bibcite{fu2011}{7}
\bibcite{nott2012}{8}
\bibcite{tran2012}{9}
\bibcite{norets2012}{10}
\bibcite{griffin06}{11}
\bibcite{dunson2007}{12}
\bibcite{chung2009}{13}
\bibcite{tokdar2010}{14}
\bibcite{SparseMoF}{15}
\bibcite{metis}{16}
\bibcite{ChenMaggioni12}{17}
\bibcite{Daubechies1992}{18}
\bibcite{stickbreaking}{19}
\bibcite{Chauveau98anautomated}{20}
\bibcite{Jung2010}{21}
\bibcite{Arden2010}{22}
\bibcite{MRCAP11}{23}
\bibcite{Mori2006}{24}
\bibcite{Autism}{25}
\bibcite{cpac}{26}
\bibcite{Zou2008}{27}
\bibcite{power}{28}
\bibcite{Breiman2001}{29}
