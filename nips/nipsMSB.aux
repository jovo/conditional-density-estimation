\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{Manifold}
\citation{Maggioni}
\citation{mixtureexperts}
\citation{jiang1999}
\citation{fan1996}
\citation{fanyim2004}
\citation{holmes2010}
\citation{fu2011}
\citation{nott2012}
\citation{tran2012}
\citation{norets2012}
\citation{griffin06}
\citation{dunson2007}
\citation{DunsonPark}
\citation{chung2009}
\citation{tokdar2010}
\citation{tran2012}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{CART}
\citation{MARS}
\citation{Bagging}
\citation{Boosting}
\citation{RandomForest}
\citation{SparseMoF}
\citation{metis}
\@writefile{toc}{\contentsline {section}{\numberline {2}Setting}{2}{section.2}}
\newlabel{section:setting}{{2}{2}{Setting\relax }{section.2}{}}
\citation{Allard2012}
\citation{ChenMaggioni12}
\newlabel{eq:linear}{{1}{3}{Setting\relax }{equation.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of our generative model and algorithm on a swissroll. The top left panel shows the manifold $\mathcal  {M}$ (a swissroll) embedded in a $p$-dimensional ambient space, where the color indicates the coordinate along the manifold, $\eta $ (only the first 3 dimensions are shown for visualization purposes). The bottom left panel shows the distribution of $Y$ as a function of $\eta $, in particular, $F_{Y|\eta }=\mathcal  {N}(\boldsymbol  {\eta },\boldsymbol  {\eta }+1)$. The middel and right panels show our estimates of $F_{Y|\boldsymbol  {\eta }}$ at scales 3 and 4, respectively, which follow from partitioning our data.}}{3}{figure.1}}
\newlabel{fig:swiss}{{1}{3}{Illustration of our generative model and algorithm on a swissroll. The top left panel shows the manifold $\mc {M}$ (a swissroll) embedded in a $p$-dimensional ambient space, where the color indicates the coordinate along the manifold, $\eta $ (only the first 3 dimensions are shown for visualization purposes). The bottom left panel shows the distribution of $Y$ as a function of $\eta $, in particular, $F_{Y|\eta }=\mc {N}(\mb {\eta },\mb {\eta }+1)$. The middel and right panels show our estimates of $F_{Y|\mb {\eta }}$ at scales 3 and 4, respectively, which follow from partitioning our data}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Goal}{3}{section.3}}
\newlabel{sec:goal}{{3}{3}{Goal\relax }{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology}{3}{section.4}}
\newlabel{sec:method}{{4}{3}{Methodology\relax }{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Ms.\nobreakspace  {}Deeds\nobreakspace  {}Framework}{3}{subsection.4.1}}
\citation{Daubechies1992}
\citation{ChenMaggioni12}
\citation{METIS}
\@writefile{toc}{\contentsline {paragraph}{Tree Decomposition}{4}{section*.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (i) Multiscale partition of the data. (ii) Path through the tree for $x_i \in \mathbb  {R}^p$. (iii) Conditional density of $y_i$ given $x_i$ defined as a convex combination of densities along the path.}}{4}{figure.2}}
\newlabel{graph}{{2}{4}{(i) Multiscale partition of the data. (ii) Path through the tree for $x_i \in \Real ^p$. (iii) Conditional density of $y_i$ given $x_i$ defined as a convex combination of densities along the path}{figure.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Embeddings}{4}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Family}{4}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Specific Choices}{4}{subsection.4.2}}
\newlabel{sub:spec}{{4.2}{4}{Specific Choices\relax }{subsection.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Tree Partition}{4}{section*.4}}
\citation{stickbreaking}
\citation{ChenMaggioni12}
\citation{Allard2012}
\citation{Allard2012}
\citation{Lawlor2012}
\citation{ChenMaggioni12}
\@writefile{toc}{\contentsline {paragraph}{Embedding}{5}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Family}{5}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Estimation}{5}{subsection.4.3}}
\newlabel{eq:prS}{{(i)}{5}{Estimation\relax }{Item.1}{}}
\citation{Chauveau98anautomated}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Predictions}{6}{subsection.4.4}}
\newlabel{ch3:predictions}{{4.4}{6}{Predictions\relax }{subsection.4.4}{}}
\newlabel{predictive:MSB}{{4.4}{6}{Predictions\relax }{subsection.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulation studies}{6}{section.5}}
\newlabel{sec:sim}{{5}{6}{Simulation studies\relax }{section.5}{}}
\@writefile{toc}{\contentsline {paragraph}{(1) Nonlinear Mixture}{7}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{(2) Linear Subspace}{7}{section*.8}}
\newlabel{eq:linear}{{3}{7}{(2) Linear Subspace\relax }{equation.5.3}{}}
\newlabel{eq:linear}{{4}{7}{(3) Union of Linear Subspaces\relax }{equation.5.4}{}}
\@writefile{toc}{\contentsline {paragraph}{(3) Union of Linear Subspaces}{7}{equation.5.4}}
\@writefile{toc}{\contentsline {paragraph}{(4) Swissroll}{7}{section*.10}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{7}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Illustrative Example}{7}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustrative example: Plot of true (red line) and estimated density ($50$th percentile: solid line, $2.5$th and $97.5$th percentiles: dashed lines) for two data points $(I, II)$ considering different training set size (a:100, b:150, c:200). }}{7}{figure.3}}
\newlabel{plotDensity}{{3}{7}{Illustrative example: Plot of true (red line) and estimated density ($50$th percentile: solid line, $2.5$th and $97.5$th percentiles: dashed lines) for two data points $(I, II)$ considering different training set size (a:100, b:150, c:200)}{figure.3}{}}
\citation{Jung2010}
\citation{Arden2010}
\citation{MRCAP11}
\citation{Mori2006}
\citation{Autism}
\citation{cpac}
\citation{Zou2008}
\citation{power}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Quantitative Comparisons}{8}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Numerical results for various simulation scenarios. Top (bottom) plots depict the relative mean-squared error (CPU time in seconds) of our approach, MSB, versus CART (red) and Lasso (black). The three simulation scenarios are: \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:linear}\unskip \@@italiccorr )}} (left), \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:MFA}\unskip \@@italiccorr )}} (middle) and \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:swiss}\unskip \@@italiccorr )}} (right). MSB outperforms both CART and Lasso in all three scenarios regardless of ambient dimension ($r_{mse}^{\mathcal  {A}}< 1$ for all $p$). MSB compute time is relatively constant as $n$ or $p$ increase, whereas Lassso's compute time increases, thus, as $n$ or $p$ increase, MSB CPU time becomes less than Lasso's. MSB was always significantly faster than CART, regardless of $n$ or $p$.}}{8}{figure.4}}
\newlabel{fig:boxplots}{{4}{8}{Numerical results for various simulation scenarios. Top (bottom) plots depict the relative mean-squared error (CPU time in seconds) of our approach, MSB, versus CART (red) and Lasso (black). The three simulation scenarios are: \eqref {eq:linear} (left), \eqref {eq:MFA} (middle) and \eqref {eq:swiss} (right). MSB outperforms both CART and Lasso in all three scenarios regardless of ambient dimension ($r_{mse}^{\mc {A}}< 1$ for all $p$). MSB compute time is relatively constant as $n$ or $p$ increase, whereas Lassso's compute time increases, thus, as $n$ or $p$ increase, MSB CPU time becomes less than Lasso's. MSB was always significantly faster than CART, regardless of $n$ or $p$}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Neuroscience Applications}{8}{subsection.6.3}}
\newlabel{sub:real}{{6.3}{8}{Neuroscience Applications\relax }{subsection.6.3}{}}
\bibstyle{unsrt}
\bibdata{nipsMSB}
\bibcite{Manifold}{1}
\bibcite{Maggioni}{2}
\bibcite{mixtureexperts}{3}
\bibcite{jiang1999}{4}
\bibcite{fan1996}{5}
\bibcite{fanyim2004}{6}
\bibcite{holmes2010}{7}
\bibcite{fu2011}{8}
\bibcite{nott2012}{9}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Real Data: Mean and standard deviations of squared error under multiscale stick-breaking (MSB), CART, Lasso and random forest (RF). Variable $r_{T}$ is the amount of time necessary to obtain predictions for all subjects, while variables $r_M$ and $r_V$ are respectively the mean and the standard deviation of amount of time necessary to obtain one point predictions.}}{9}{table.1}}
\newlabel{real}{{1}{9}{Real Data: Mean and standard deviations of squared error under multiscale stick-breaking (MSB), CART, Lasso and random forest (RF). Variable $r_{T}$ is the amount of time necessary to obtain predictions for all subjects, while variables $r_M$ and $r_V$ are respectively the mean and the standard deviation of amount of time necessary to obtain one point predictions}{table.1}{}}
\bibcite{tran2012}{10}
\bibcite{norets2012}{11}
\bibcite{griffin06}{12}
\bibcite{dunson2007}{13}
\bibcite{DunsonPark}{14}
\bibcite{chung2009}{15}
\bibcite{tokdar2010}{16}
\bibcite{CART}{17}
\bibcite{MARS}{18}
\bibcite{Bagging}{19}
\bibcite{Boosting}{20}
\bibcite{RandomForest}{21}
\bibcite{SparseMoF}{22}
\bibcite{metis}{23}
\bibcite{stickbreaking}{24}
\bibcite{Chauveau98anautomated}{25}
\bibcite{Jung2010}{26}
\bibcite{Arden2010}{27}
\bibcite{MRCAP11}{28}
\bibcite{Mori2006}{29}
\bibcite{Autism}{30}
\bibcite{cpac}{31}
\bibcite{Zou2008}{32}
\bibcite{power}{33}
