\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{Manifold}
\citation{Maggioni}
\citation{mixtureexperts}
\citation{jiang1999}
\citation{fan1996}
\citation{fanyim2004}
\citation{holmes2010}
\citation{fu2011}
\citation{nott2012}
\citation{tran2012}
\citation{norets2012}
\citation{griffin06}
\citation{dunson2007}
\citation{DunsonPark}
\citation{chung2009}
\citation{tokdar2010}
\citation{tran2012}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{CART}
\citation{MARS}
\citation{Bagging}
\citation{Boosting}
\citation{RandomForest}
\citation{SparseMoF}
\citation{metis}
\@writefile{toc}{\contentsline {section}{\numberline {2}Setting}{2}{section.2}}
\newlabel{sec:setting}{{2}{2}{Setting\relax }{section.2}{}}
\citation{Allard2012}
\citation{ChenMaggioni12}
\newlabel{eq:linear}{{1}{3}{Setting\relax }{equation.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of our generative model and algorithm on a swissroll. The top left panel shows the manifold $\mathcal  {M}$ (a swissroll) embedded in a $p$-dimensional ambient space, where the color indicates the coordinate along the manifold, $\eta $ (only the first 3 dimensions are shown for visualization purposes). The bottom left panel shows the distribution of $Y$ as a function of $\eta $, in particular, $F_{Y|\eta }=\mathcal  {N}({\eta },{\eta }+1)$. The middle and right panels show our estimates of $F_{Y|{\eta }}$ at scales 3 and 4, respectively, which follow from partitioning our data. Sample size was $n=10\tmspace  +\thinmuskip {.1667em}000$.}}{3}{figure.1}}
\newlabel{fig:swiss}{{1}{3}{Illustration of our generative model and algorithm on a swissroll. The top left panel shows the manifold $\mc {M}$ (a swissroll) embedded in a $p$-dimensional ambient space, where the color indicates the coordinate along the manifold, $\eta $ (only the first 3 dimensions are shown for visualization purposes). The bottom left panel shows the distribution of $Y$ as a function of $\eta $, in particular, $F_{Y|\eta }=\mc {N}({\eta },{\eta }+1)$. The middle and right panels show our estimates of $F_{Y|{\eta }}$ at scales 3 and 4, respectively, which follow from partitioning our data. Sample size was $n=10\,000$}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Goal}{3}{section.3}}
\newlabel{sec:goal}{{3}{3}{Goal\relax }{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology}{3}{section.4}}
\newlabel{sec:method}{{4}{3}{Methodology\relax }{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Ms.\nobreakspace  {}Deeds\nobreakspace  {}Framework}{3}{subsection.4.1}}
\newlabel{sub:method}{{4.1}{3}{\msd ~Framework\relax }{subsection.4.1}{}}
\citation{Daubechies1992}
\citation{ChenMaggioni12}
\citation{METIS}
\@writefile{toc}{\contentsline {paragraph}{Tree Decomposition}{4}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{Embeddings}{4}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Family}{4}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Specific Choices}{4}{subsection.4.2}}
\newlabel{sub:spec}{{4.2}{4}{Specific Choices\relax }{subsection.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Tree Partition}{4}{section*.4}}
\citation{stickbreaking}
\citation{ChenMaggioni12}
\citation{Allard2012}
\citation{Allard2012}
\citation{Lawlor2012}
\citation{ChenMaggioni12}
\@writefile{toc}{\contentsline {paragraph}{Embedding}{5}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Family}{5}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Estimation}{5}{subsection.4.3}}
\newlabel{eq:prS}{{(i)}{5}{Estimation\relax }{Item.1}{}}
\citation{Chauveau98anautomated}
\citation{Jung2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Predictions}{6}{subsection.4.4}}
\newlabel{ch3:predictions}{{4.4}{6}{Predictions\relax }{subsection.4.4}{}}
\newlabel{predictive:MSB}{{4.4}{6}{Predictions\relax }{subsection.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Simulation Studies}{6}{subsection.4.5}}
\newlabel{sec:sim}{{4.5}{6}{Simulation Studies\relax }{subsection.4.5}{}}
\citation{Arden2010}
\citation{MRCAP11}
\citation{Mori2006}
\citation{Autism}
\citation{cpac}
\citation{Zou2008}
\citation{power}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Neuroscience Applications}{7}{subsection.4.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Evaluation Criteria}{7}{subsection.4.7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{7}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Illustrative Example}{7}{subsection.5.1}}
\newlabel{sub:ill}{{5.1}{7}{Illustrative Example\relax }{subsection.5.1}{}}
\citation{Brieman2001}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustrative example suggesting that our posterior estimates of the conditional density are converging as $n$ increases even when $F_{Y|\eta }$ is highly nonlinear and $F_{X|\eta }$ is very high-dimensional. We simulated data according to Model (1) with parameters $(\mu _1,\sigma _1)=(-2,1)$, $(\mu _2,\sigma _2)=(2,1)$, $\sigma _x=0.01$, and $c=20$ for different sample sizes. True (red) and estimated (black) density ($50$th percentile: solid line, $2.5$th and $97.5$th percentiles: dashed lines) for two data points $(I, II)$ considering different training set size (a: $n=100$, b: $n=150$, c: $n=200$). }}{8}{figure.2}}
\newlabel{plotDensity}{{2}{8}{Illustrative example suggesting that our posterior estimates of the conditional density are converging as $n$ increases even when $F_{Y|\eta }$ is highly nonlinear and $F_{X|\eta }$ is very high-dimensional. We simulated data according to Model (1) with parameters $(\mu _1,\sigma _1)=(-2,1)$, $(\mu _2,\sigma _2)=(2,1)$, $\sigma _x=0.01$, and $c=20$ for different sample sizes. True (red) and estimated (black) density ($50$th percentile: solid line, $2.5$th and $97.5$th percentiles: dashed lines) for two data points $(I, II)$ considering different training set size (a: $n=100$, b: $n=150$, c: $n=200$)}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Quantitative Comparisons for Simulated Data}{8}{subsection.5.2}}
\newlabel{sub:sim}{{5.2}{8}{Quantitative Comparisons for Simulated Data\relax }{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Quantitative Comparisons for Neuroscience Applications}{8}{subsection.5.3}}
\newlabel{sub:real}{{5.3}{8}{Quantitative Comparisons for Neuroscience Applications\relax }{subsection.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  {\color  {blue}{\it  the panels will be re-arranged shortly to reflect the legend.}} Numerical results for various simulation scenarios. Top (bottom) plots depict the relative mean-squared error (CPU time in seconds) of MSB (our approach), versus CART (red) and Lasso (black). The three simulation scenarios are: linear subspaces (left), union of linear subspaces (middle) and the swissroll (right). MSB outperforms both CART and Lasso in all three scenarios regardless of ambient dimension ($r_{mse}^{\mathcal  {A}}< 1$ for all $p$). MSB compute time is relatively constant as $n$ or $p$ increase, whereas Lassso's compute time increases, thus, as $n$ or $p$ increase, MSB CPU time becomes less than Lasso's. MSB was always significantly faster than CART, regardless of $n$ or $p$. }}{9}{figure.3}}
\newlabel{fig:boxplots}{{3}{9}{\dd {the panels will be re-arranged shortly to reflect the legend.} Numerical results for various simulation scenarios. Top (bottom) plots depict the relative mean-squared error (CPU time in seconds) of MSB (our approach), versus CART (red) and Lasso (black). The three simulation scenarios are: linear subspaces (left), union of linear subspaces (middle) and the swissroll (right). MSB outperforms both CART and Lasso in all three scenarios regardless of ambient dimension ($r_{mse}^{\mc {A}}< 1$ for all $p$). MSB compute time is relatively constant as $n$ or $p$ increase, whereas Lassso's compute time increases, thus, as $n$ or $p$ increase, MSB CPU time becomes less than Lasso's. MSB was always significantly faster than CART, regardless of $n$ or $p$}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Neuroscience application quantitative performance comparisons. Squared error predictive accuracy per subject (using leave-one-out) was computed. we report the mean and standard deviation (s.d.) across subjects of squared error, as well as CPU time (in seconds). We compare multiscale stick-breaking (MSB), CART, Lasso and random forest (RF). MSB outperforms all the competitors in terms of predictive accuracy and scalability for both applications.}}{9}{table.1}}
\newlabel{real}{{1}{9}{Neuroscience application quantitative performance comparisons. Squared error predictive accuracy per subject (using leave-one-out) was computed. we report the mean and standard deviation (s.d.) across subjects of squared error, as well as CPU time (in seconds). We compare multiscale stick-breaking (MSB), CART, Lasso and random forest (RF). MSB outperforms all the competitors in terms of predictive accuracy and scalability for both applications}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{9}{section.6}}
\newlabel{sec:disc}{{6}{9}{Discussion\relax }{section.6}{}}
\citation{Allard2012}
\bibstyle{unsrt}
\bibdata{nipsMSB}
\bibcite{Manifold}{1}
\bibcite{Maggioni}{2}
\bibcite{mixtureexperts}{3}
\bibcite{jiang1999}{4}
\bibcite{fan1996}{5}
\bibcite{fanyim2004}{6}
\bibcite{holmes2010}{7}
\bibcite{fu2011}{8}
\bibcite{nott2012}{9}
\bibcite{tran2012}{10}
\bibcite{norets2012}{11}
\bibcite{griffin06}{12}
\bibcite{dunson2007}{13}
\bibcite{DunsonPark}{14}
\bibcite{chung2009}{15}
\bibcite{tokdar2010}{16}
\bibcite{CART}{17}
\bibcite{MARS}{18}
\bibcite{Bagging}{19}
\bibcite{Boosting}{20}
\bibcite{RandomForest}{21}
\bibcite{SparseMoF}{22}
\bibcite{metis}{23}
\bibcite{stickbreaking}{24}
\bibcite{Chauveau98anautomated}{25}
\bibcite{Jung2010}{26}
\bibcite{Arden2010}{27}
\bibcite{MRCAP11}{28}
\bibcite{Mori2006}{29}
\bibcite{Autism}{30}
\bibcite{cpac}{31}
\bibcite{Zou2008}{32}
\bibcite{power}{33}
