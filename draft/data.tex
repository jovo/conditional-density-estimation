We assessed the predictive performance of the proposed method on two very different neuroimaging datasets. First, we consider a structural connectome dataset collected at the Mind Research Network.  Data were collected as described previously \cite{??}.
% ,and structural connectomes were estimated using the Magnetic Resonance Connectome Automated Pipeline \cite{MRCAP}.  
% Each of the $108$ children underwent both multimodal imaging and a battery of cognitive assessments.  
We investigated the extent to which we could predict creative (as measured via the Composite Creativity Index \cite{Arden2010}).  
% The first dataset consists of a measurement of creativity observed for $108$ subjects. We would like to predict the value of creativity based on the number of connections between different cortical regions of the brain. 
For each subject, we estimate a $70$ vertex undirected weighted brain-graph using the Magnetic Resonance Connectome Automated Pipeline \cite{MRCAP11} from diffusion tensor imaging data \cite{Mori2006}.
% a brain graph involving $70$ cortical regions was observed. 
We therefore let each $x_i \in \Real^p$ correspond to logarithm of each weighted edge; because our graphs are undirected and lack self-loops, we have a total of $\binom{70}{2}=2415$ potential weighted edges.
The vector of covariates consists in the \jovo{which log?} logarithm of the total number of connections between all pairs of cortical regions, i.e. $p=2,415$. 

The second dataset comes from a resting-state functional magnetic resonance experiment as part of the Autism Brain Imaging Data Exchange.  We selected \jovo{which site} for analysis.  Each brain-image was processed using the Configurable Pipepline for Analysis of Connectomes \cite{cpac}. For each subject we computed a measure of normalized power at each voxel called fALFF \cite{Zou2008}.  fALFF is a highly nonlinear transformation of the time-series data, previously demonstrated to be a reliable property of such data.  To ensure the existence of nonlinear signal relating these predictors, we let $y_i$ correspond to an estimate of overall head motion in the scanner, called mean framewise displacement (FD) computed as described in \cite{power}.  We utilized a gray matter mask to consider only the voxels with high probability of being gray matter. Thus, for each of $56$ subjects, we let $x_i \in \Real^{300,000}$ be the fALFF of all gray matter voxels.

% Our interest was predicting the head motion measurement based on $3$D brain images involving about one million of pixels. These $3$D matrices were vectorized and considered as predictors in our model. In order to reduce the dimensionality of the predictor space the data was re-processed using a brain mask and a vector of about $300,000$ predictors  was obtained.

For the analysis, all variables were normalized by subtracting the mean and dividing by the variance \jovo{do you mean standard deviation? i imagine so.}. The same prior specification and Gibbs sampler as in \S 5 was  utilized. Table \ref{real} shows mean and variance squared error based on leave-one-out predictions. Variable $t_{T}$ is the amount of time necessary to obtain predictions for all subjects, while variables $t_M$ and $t_V$ are respectively the mean and the variance \jovo{lets report standard deviation, it is on the same scale as mean, unlike variance, which is related to mean squared.} of amount of time necessary to obtain one point predictions.

For the first data example, we compared our approach (multiresolution stick-breaking; MSB) to CART, lasso and random forests. 
For CART, lass, and random forests, we use standard R packages downloaded from CRAN \jovo{name the packages explicitly; actually put this up there when you first introduct them for the simulations}.
Table \ref{real} shows that MSB outperforms all the competitors in terms of mean square error; this is in addition to yielding an estimate of the entire conditional density for each $y_i$.  It is also significantly faster that random forests, the next closest competitor, and faster than lasso.  For this relatively low-dimensional example, CART is reasonably fast.  

 % random forest in terms of mean squared error, and is associated to a much lower CPU time. This real data application does not involve a huge number of predictors so that computationally our model performs almost as well as lasso and CART. However, as  shown in section 5, our model can scale substantially better than all other models to huge number of features. \\
 
 For the second data application, given the huge dimensionality of the predictor space, we were unable to get either CART or random forest to run to completion, yielding memory faults on our workstation \jovo{add computer details, how may cores, how much ram, etc.}.  We thus only compare performance to lasso.  As in the previous example, MSB outperforms lasso in terms of predictive accuracy measured via mean-squared error, and \emph{significantly} outperforms lasso in terms of computational time.  
 % and the poor scalability of CART and random forest, the comparison was made only with lasso. As shown in table \ref{real}, our approach is more efficient and accurate than lasso in predicting the response variable. 
Figure \ref{fig:real} shows the plot of CPU time used to predict each one of the $56$ subjects involved in the experiment. The time needed to compute quantities utilized in all subject predictions was divided equally across subjects. Clearly, our approach is able to improve the computational time by up to five orders of magnitude. 

