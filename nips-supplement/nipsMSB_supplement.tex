
\documentclass{article} % For LaTeX2e
\usepackage{nips12submit_e,times}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09

% For figures
\usepackage{multirow}

\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}
\usepackage{subfigure}
% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2013} with
% \usepackage[nohyperref]{icml2013} above.
\usepackage{hyperref}
\nocite{*}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% jovo added stuff
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\mbX}{\mathbf{X}}
\newcommand{\mbY}{\mathbf{Y}}
\newcommand{\Real}{\mathbb{R}}
\providecommand{\mh}[1]{\hat{#1}}
\providecommand{\mb}[1]{\boldsymbol{#1}}
\providecommand{\mc}[1]{\mathcal{#1}}
\providecommand{\mt}[1]{\widetilde{#1}}
\newcommand{\from}{{\ensuremath{\colon}}}  % :
\usepackage{amsmath,amssymb,amsfonts}
\newcommand{\conv}{\rightarrow}

\newcommand{\efoo}{\end{footnotesize}}
\newcommand{\bfoo}{\begin{footnotesize}}
\renewcommand{\labelenumi}{\theenumi}
\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\floatname{algorithm}{Pseudocode}
\providecommand{\norm}[1]{\left \lVert#1 \right  \rVert}
\newcommand{\T}{^{\ensuremath{\mathsf{T}}}}           % transpose


% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
% \usepackage[accepted]{icml2013}

% jovo additions
\usepackage{color}
\newcommand{\jovo}[1]{{\color{magenta}{\it #1}}}
\newcommand{\msd}{Ms.~Deeds}
% \newcommand{\Real}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\II}{\mathbb{I}}
\usepackage{color}
\newcommand{\dd}[1]{{\color{blue}{\it #1}}}
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{result}[theorem]{Result}
\newtheorem{definition}[theorem]{Definition}
\providecommand{\mtc}[1]{\widetilde{\mathcal{#1}}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\title{Supplementary material for: Multiresolution dictionary learning for conditional distributions}


%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle

% \section{Full conditionals}
% Introduce the latent variable $S_i \in \{1,\ldots,k\}$, for $i=1,\ldots,n$, denoting the multiscale level used by the $i$th subject.  Assuming data are normalized prior to analysis, we let $\mu \sim \mc{N}(0,I)$ and $\sigma=\mc{IG}(a,b)$ for the means and variances of the dictionary densities. Let $n_{B_j}$ be  the number of observations allocated to node $B_j$ . Each Gibbs sampler iteration can be summarized in the following steps.
% \begin{enumerate}
% \item Update $S_i$ by sampling from the multinomial full conditional with 
% \[\mbox{Pr}( S_i = j\, |\, -) = \frac{ \pi_{B_j(x_i)}f_{B_j(x_i)}(y_i) }{ \sum_{h=1}^k \pi_{B_h(x_i)}f_{B_h(x_i)}(y_i) } \label{eq:prS}\]
% \item Update stick-breaking random variable $V_{B_j(x_i)}$, for $j=1, \ldots, k$ and $i=1, \ldots, n$, from $\mbox{Beta}(\beta_p,\alpha_p)$ with $\beta_p=1+n_{B_j}$ and $\alpha_p=\alpha+\sum_{B_h(x_i) \in de\{B_j(x_i)\}} n_{B_h(x_i)}$.
% \item Update $(\mu_{B_j(x_i)},\sigma_{B_j(x_i)})$ by sampling from
% \[  \mu_{B_j} \sim \mc{N}\left(\bar{y}_{B_j} n_{B_j}/\sigma_{B_j},(1+n_{B_j}/\sigma_{B_j})^{-1}\right)\]
% \[ \sigma_{B_j} \sim \mc{IG}\left(a_{\sigma},b+0.5\sum_{\{i: S_i=j,x_i \in B_j\}} \left(y_{i}-\mu_{B_j}\right)^2\right)\]
% with $a_{\sigma}=a+n_{B_j}/2$, $\bar{y}_{B_j}$ being the average of the observation $\{y_i\}$ allocated to node $B_j$.
% \end{enumerate}


\section{Partition Tree Schematic}

\begin{figure}[!htbp] 

\setlength{\unitlength}{2mm}
\begin{picture}(30, 26)

 \put(9, 22){\bfoo(i)\efoo}
 
 \put(20, 22){$\mathcal{\mc{W}}_{11} = \mathcal{\mc{W}}$}
\put(20, 20){\vector(-1, -1){3}}
\put(20, 20){\vector(1, -1){3}}

 \put(14, 14){$\mathcal{\mc{W}}_{21}$}
 \put(23, 14){$\mathcal{\mc{W}}_{22}$}
 
\put(14, 12){\vector(-1, -1){3}}
\put(14, 12){\vector(1, -1){3}}

\put(25, 12){\vector(-1, -1){3}}
\put(25, 12){\vector(1, -1){3}}

 \put(9, 6){$\mathcal{\mc{W}}_{31}$}
 \put(16, 6){$\mathcal{\mc{W}}_{32}$}
 
 \put(20,6){$\mathcal{\mc{W}}_{33}$}
 \put(28, 6){$\mathcal{\mc{W}}_{34}$}
  
  \put(13, 3){$\ldots$}
  \put(25, 3){$\ldots$}

% -- square
\put(6, 25){\line(1, 0){56}}
\put(6, 25){\line(0, -1){27}}
\put(6, -2){\line(1, 0){56}}
\put(62, 25){\line(0, -1){27}}

%% right graph

\put(36, 14){$w_i \in \mc{W}$}
\put(39, 13){\line(0, -1){1}}
\put(39, 12){\vector(1, 0){5}}

\put(37, 22){\bfoo(ii)\efoo}
  \put(48, 22){$\mc{W}_{11}$}
\put(48, 20){\vector(-1, -1){3}}
\put(48, 20){\vector(1, -1){3}}

 \put(51, 14){$\mc{W}_{22}$}
 \put(53, 12){\vector(-1, -1){3}}
\put(53, 12){\vector(1, -1){3}}

 \put(48,6){$\mc{W}_{33}$}
  \put(53, 3){$\ldots$}
  \put(18, -0.5){\bfoo(iii)\efoo}
 \put(22, -0.5){$f(y_i|x_i)=p_{11}f_{11}+p_{22}f_{22}+p_{33}f_{33}+ \ldots$}

\end{picture} \caption{(i) Multiscale partition of the data. (ii) Path through the tree for $x_i \in \Real^p$. (iii) Conditional density of $y_i$ given $x_i$ defined as a convex combination of densities along the path.}\label{graph}
\end{figure}


\section{Predictions}\label{ch3:predictions}

Consider the case we want to predict the response $y^{*}$ for a future observation based on predictors $x^{*}$ and previous observations $(x^{(n)},y^{(n)})$ with $x^{(n)}=(x_1,  \ldots, x_n)$ and $y^{(n)}=(y_1,  \ldots, y_n)$. Because the partitioning strategy that we adopted lacks an elegant out-of-sample embedding function (unlike other paritioning strategies), we adopt a Voronoi expansion procedure by which  the new predictors $x^{*}$ are allocated to $C_{j,k}$'s having the closest centers with respect to $\rho_W$ (we considered the Euclidean distance). Summaries of the predictive density of $y^{*}$ will be computed as follows:

(i) allocate predictors $x^{*}$ to $C_{j,k}$'s having the closest centers with respect to $\rho_W$

(ii) run the Gibbs sampler for $S$ iterations, and at the $s$th  iteration:

 \hspace{5pt} a) sample parameters $\{\sigma^{(s)}_{j,k_j}, \mu^{(s)}_{j,k_j}, \pi^{(s)}_{j,k_j} \}_{j\in\ZZ,k_j \in\mc{K}_j}$ from the posterior, i.e.  $p(.| x^{(n)},y^{(n)})$
   
 \hspace{5pt}   b) sample $\hat{y}^*_{s}$ from 
 $$ \sum_{j \in \ZZ} \pi^{(s)}_{j,k_j(x^{*})} \mc{N}\left(\mu^{(s)}_{j,k_j(x^{*})},\sigma^{(s)}_{j,k_j(x^{*})}\right)$$
   
 (iii) given the sequence $\left\lbrace  \hat{y}^*_{s}\right\rbrace_{s=1}^S$, summaries of the predictive density such as mean, variance and quantiles can be computed.



\section{Synthetic examples}
\subsection{Competitor Algorithms}

As we are unaware of other methods, even frequentist, that estimate posteriors with such high-dimensional predictors, we compare point estimates of our approach with other moderately regression algorithms.  In particular, we elected to compare against lasso, classification and regression trees (CART), Random Forest (RF) and principal component (PC) regression. The lasso regularization parameter and the number of principal components for PC regression were chosen based on  the Akaike information criterion (AIC). For all algorithms, standard Matlab packages were utilized. 

\subsection{Additional results}
Tables \ref{table:linear1}, \ref{table:mfa} and \ref{table:swiss} show results concerning example 2, 3, and 4 in section 4.4. Each table reports mean squared errors and the mean of amount of time necessary to obtain one point predictions. In particular, table \ref{table:linear1} shows results concerning example 3 (linear subspace) for different number of factors ($d=5,10$), table \ref{table:mfa} shows results concerning example 4 (union of linear subspaces) for different number of mixture components ($G=5, 10$), while table \ref{table:swiss} shows results for example 2 (swissroll). As shown, in almost all data scenario, our model is able to perform as well as or better than the model associated to the lowest mean squared error and can scale substantially better than others to high dimensional predictors. 


\begin{table}[t]
\caption{Linear subspace: Mean and standard deviations of squared errors under multiscale stick-breaking (MSB), CART and Lasso for sample size 50 and 100 for different simulation scenarios.}\label{table:linear1}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lllcccccc}
\hline
&&&\multicolumn{3}{c}{$d=5$}&\multicolumn{3}{c}{$d=10$}\\
$p$&$n$& & msb&cart&lasso & msb&cart&lasso \\
\\
\multirow{3}{*}{$50k$}&\multirow{3}{*}{50}&\bfoo mse\efoo&0.18&0.31&0.25&0.22&0.58&0.22\\
&&\bfoo std\efoo &0.32&0.30&0.42&0.24&0.54&0.30\\
&&\bfoo time\efoo &3&2&1&3&3&1\\

\\
\multirow{3}{*}{$50k$}&\multirow{3}{*}{100}&\bfoo mse\efoo&0.18&0.27&0.26&0.20&0.41&0.52\\
&&\bfoo std\efoo & 0.26&0.42&0.46&0.23&0.46&0.78\\
&&\bfoo time\efoo &5&5& 2&5&5&1\\

\\
\multirow{3}{*}{$100k$}&\multirow{3}{*}{50}&\bfoo mse\efoo&$0.35$&$0.45$&$0.89$&$0.16$&$0.33$&$0.20$\\
&&\bfoo std\efoo &$0.53$ &$0.77$&$1.04$&$0.21$&$0.46$&$0.31$\\
&&\bfoo time\efoo &$3$&$25$&$2$&$3$&$27$&$2$\\
\\
\multirow{3}{*}{$100k$}&\multirow{3}{*}{100}&\bfoo mse\efoo&$0.43$&$0.88$&$0.52$&$0.17$&$0.50$&$0.31$\\
&&\bfoo std\efoo &$0.59$ &$1.29$&$0.70$&$0.24$ &$0.75$&$0.49$\\
&&\bfoo time\efoo &$7$&$50$&$5$&$7$&$51$&$5$\\
\\
\multirow{3}{*}{$500k$}&\multirow{3}{*}{50}&\bfoo mse\efoo&0.11&0.16&0.15&0.83&2.26&0.92\\
&&\bfoo std\efoo&$0.15$ &0.24&0.19&1.01&2.60&3.69\\
&&\bfoo time\efoo &5&90&11&5&121&10\\


\\
\multirow{3}{*}{$500k$}&\multirow{3}{*}{100}&\bfoo mse\efoo&0.003&0.17&0.08&0.13&1.37&1.06\\
&&\bfoo std\efoo &0.16&0.23&0.13&1.12&1.81&1.50\\
&&\bfoo time\efoo &10&214&43&8&227&42\\

\\
\multirow{3}{*}{$700k$}&\multirow{3}{*}{50}&\bfoo mse\efoo&1.70&1.48&1.47&0.66&1.65&1.07\\
&&\bfoo std\efoo &2.18&2.47&1.63&0.87&1.49&0.95\\
&&\bfoo time\efoo &6&121&12&7&151&13\\

\\
\multirow{3}{*}{$700k$}&\multirow{3}{*}{100}&\bfoo mse\efoo&0.69&1.36&0.82&0.78&1.52&1.43\\
&&\bfoo std\efoo &0.94&1.47&1.28&1.03&1.34&2.11\\
&&\bfoo time\efoo &13&321&41&12&325&44\\

 \hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}




\begin{table}[t]
\caption{Union of linear subspaces: Mean and standard deviations of squared errors under multiscale stick-breaking (MSB), CART and Lasso for different sample sizes for different simulations sampled from a mixture of factor analyzers}\label{table:mfa}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lllcccccc}
\hline
&&&\multicolumn{3}{c}{$G=10$}&\multicolumn{3}{c}{$G=5$}\\
$p$&$n$& sim& msb&cart&lasso & msb&cart&lasso\\
\\
\multirow{3}{*}{$50k$}&\multirow{3}{*}{100}&mse&0.23&0.42&0.36&0.17&0.43&0.22\\
&&std & 0.34 &0.59&0.43&0.18&0.69&0.23\\
&&time &5&24 & 3&7&27&3\\

\\
\multirow{3}{*}{$50k$}&\multirow{3}{*}{200}&mse&0.23 &0.42 &0.27&0.17&0.22&0.20\\
&&std & 0.33& 0.56&0.23&0.19&0.38&0.25\\
&&time & 10 &51&8&12&56&7\\

\\
\multirow{3}{*}{$100k$}&\multirow{3}{*}{100}&mse&0.67&1.35&1.32&0.15&0.17&0.22\\
&&std & 1.04&2.26&1.36&0.23&0.19&0.23\\
&&time &9&47&6&6&44&5\\

\\
\multirow{3}{*}{$100k$}&\multirow{3}{*}{200}&mse&0.64&1.37&0.85&0.15&0.26&0.15\\
&&std &0.95 &1.77&1.29&0.24&0.42&0.24\\
&&time &15&99&15&11&89&15\\
\\
\multirow{3}{*}{$300k$}&\multirow{3}{*}{100}&mse& 0.26&0.39&0.31&0.63&1.40&1.01\\
&&std &0.39&0.51&0.52&0.80 &1.24& 1.46 \\
&&time &9.28&125&18&9 &145& 17\\
\\
\multirow{3}{*}{$300k$}&\multirow{3}{*}{200}&mse&0.25&0.47&0.26&0.63&1.17&0.92\\
&&std &0.36&0.88&0.43 & 0.80&2.11&1.04 \\
&&time &15&262&40&13&283&43\\


\\
\multirow{3}{*}{$300k$}&\multirow{3}{*}{300}&mse&0.25&0.30&0.30&0.62&1.42&0.70\\
&&std &0.36&0.41&0.48&0.89&1.85&0.94\\
&&time &15&463&73&16&465&89\\

\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\begin{table}[t]
\caption{Swissroll: Mean and standard deviations of squared errors under multiscale stick-breaking (MSB), CART and Lasso for different sample sizes for different simulation scenarios.}\label{table:swiss}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lllcccccc}
\hline
$p$&$n$& & msb&cart& lasso\\
\multirow{3}{*}{$100k$}&\multirow{3}{*}{50}&mse &$0.24$&$0.44$&$0.25$\\
&&std & $0.24$ & $0.42$&$0.29$\\
&&time & 3 & $22$&$2$ \\

\\
\multirow{3}{*}{$100k$}&\multirow{3}{*}{100}&mse &$0.24$ & $0.43$&$0.17$\\
&&std & $0.26$&$0.55$&$0.22$\\
&&time&$6$&$48$&$7$\\

\\

\multirow{3}{*}{$200k$}&\multirow{3}{*}{50}&mse &$0.24$&$0.67$&$0.29$\\
&&std & $0.23$ & $0.50$& $0.29$\\
&&time & 4& $38$& $5$ \\
\\
\multirow{3}{*}{$200k$}&\multirow{3}{*}{100}&mse &$0.25$&$0.78$&$0.33$\\
&&std & $0.26$ & $0.74$&$0.36$\\
&&time &6 &$96$&$13$\\
\\

\multirow{3}{*}{$500k$}&\multirow{3}{*}{50}&mse &$0.17$&$0.47$&$0.23$\\
&&std & $0.23$ & $0.43$&$0.22$\\
&&time &$5$ &$126$&$10$  \\

\\
\multirow{3}{*}{$500k$}&\multirow{3}{*}{100}&mse &$0.17$&$0.33$&$0.19$\\
&&std & $0.21$ &$0.46$ &$0.23$\\
&&time &$11$ &$230$&$25$\\



\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\bibliographystyle{unsrt} 
\bibliography{nipsMSB_supplement} 

\end{document}

