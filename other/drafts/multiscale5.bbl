\begin{thebibliography}{13}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bishop \& Svensen(2003)Bishop and Svensen]{BHierMoF}
Bishop, C.M. and Svensen, M.
\newblock {Bayesian Hierarchical mixtures of experts}.
\newblock \emph{{Nineteenth Conference on Uncertainty in Artificial
  intelligence}}, pp.\  57--64, 2003.

\bibitem[Breiman(1996)]{Bagging}
Breiman, L.
\newblock {Bagging predictors}.
\newblock \emph{{Machine Learning}}, 24:\penalty0 123–140, 1996.

\bibitem[Breiman(2001)]{RandomForest}
Breiman, L.
\newblock {Random Forests}.
\newblock \emph{{Machine Learning}}, 45:\penalty0 5--32, 2001.

\bibitem[Breiman et~al.(1984)Breiman, Friedman, Stone, and Olshen]{CART}
Breiman, L., Friedman, J., Stone, C.~J., and Olshen, R.~A.
\newblock \emph{Classification and regression trees}.
\newblock Chapman $\&$ Hall/CRC, 1984.

\bibitem[Chauveau \& Diebolt(1998)Chauveau and Diebolt]{Chauveau98anautomated}
Chauveau, Didier and Diebolt, Jean.
\newblock An automated stopping rule for mcmc convergence assessment.
\newblock \emph{Computational Statistics}, 14:\penalty0 419--442, 1998.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and Hinton]{mixtureexperts}
Jacobs, R.~A., Jordan, M.~I., Nowlan, S.~J., and Hinton, G.~E.
\newblock {Adaptive mixture of local experts}.
\newblock \emph{{Neural Computation}}, 3:\penalty0 79--87, 1991.

\bibitem[Jordan \& Jacobs(1994)Jordan and Jacobs]{HierMoF}
Jordan, M.~I. and Jacobs, R.~A.
\newblock {Hierarchical mixtures of experts and the EM algorithm}.
\newblock \emph{{Neural Computation}}, 6:\penalty0 181--214, 1994.

\bibitem[Karypis \& Kumar(1999)Karypis and Kumar]{metis}
Karypis, G. and Kumar, V.
\newblock {A fast and high quality multilevel scheme for partitioning irregular
  graphs}.
\newblock \emph{{SIAM Journal on Scientific Computing 20}}, 1:\penalty0
  359–392, 1999.

\bibitem[Meeds \& Osindero(2006)Meeds and Osindero]{AltInfMoE}
Meeds, E. and Osindero, S.
\newblock {Bayesian Hierarchical mixtures of experts}.
\newblock \emph{{Advances in Neural Information Processing Systems}}, 2006.

\bibitem[Mossavat \& Amft(2011)Mossavat and Amft]{SparseMoF}
Mossavat, I. and Amft, O.
\newblock {Sparse bayesian hierarchical mixture of experts}.
\newblock \emph{{IEEE Statistical Signal Processing Workshop (SSP)}}, 2011.

\bibitem[Rasmussen \& Ghahramani(2002)Rasmussen and Ghahramani]{infiniteMoE}
Rasmussen, C.~E. and Ghahramani, Z.
\newblock {Infinite mixtures of Gaussian process experts}.
\newblock \emph{{Advances in neural information processing systems 14}}, 2002.

\bibitem[Sethuraman(1994)]{stickbreaking}
Sethuraman, J.
\newblock {A Constructive Deﬁnition of Dirichlet Priors}.
\newblock \emph{{Statistica Sinica}}, 4:\penalty0 639--650, 1994.

\bibitem[Shapire et~al.(1998)Shapire, Freund, Bartlett, and Lee]{Boosting}
Shapire, R., Freund, Y., Bartlett, P., and Lee, W.
\newblock {Boosting the margin: A new explanation for the effectiveness of
  voting methods}.
\newblock \emph{{Annals of Statistics}}, 26:\penalty0 1651–1686, 1998.

\end{thebibliography}
