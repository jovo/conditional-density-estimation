%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2013 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2013,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{multirow}

\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2013} with
% \usepackage[nohyperref]{icml2013} above.
\usepackage{hyperref}
\nocite{*}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2013} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
% \usepackage[accepted]{icml2013}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Multiresolution dictionary learning for conditional distributions}

\begin{document} 

\twocolumn[
\icmltitle{Multiresolution dictionary learning for conditional distributions}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2013
% package.

\icmlauthor{Francesca Petralia}{fp12@duke.edu}
\icmladdress{Department of Statistical Science, Box 90251, Duke University, Durham, North Carolina 27708, U.S.A.}
\icmlauthor{Joshua Vogelstein}{joshuav@jhu.edu}
\icmladdress{Department of Statistical Science, Box 90251, Duke University, Durham, North Carolina 27708, U.S.A.}
\icmlauthor{David B. Dunson}{dunson@stat.duke.edu}
\icmladdress{Department of Statistical Science, Box 90251, Duke University, Durham, North Carolina 27708, U.S.A.}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Density regression; Dictionary learning, Manifold learning; Mixture of experts; Multiresolution stick-breaking; Nonparametric}

\vskip 0.3in
]

\begin{abstract} 
Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem.  In many settings it is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional with a distribution concentrated near a lower-dimensional subspace or manifold.  We propose a  multiresolution model based on a novel stick-breaking prior placed on the dictionary weights.  The algorithm scales efficiently to massive numbers of features, and can be implemented efficiently with slice sampling.  State of the art predictive performance is demonstrated for toy examples and an important application to predicting drug response to anti-depressants.
\end{abstract} 

Key words: Density regression; Dictionary learning, Manifold learning; Mixture of experts; Multiresolution stick-breaking; Nonparametric 

\section{Introduction}


Massive datasets arise from a variety of sources including neurological, image, video and biological applications. In all these applications, common models utilized for density estimation, classification, variable selection and predictions fail to be efficient and cannot be applied. Dealing with large amounts of data requires the introduction of new models able to process the data accurately and efficiently. In this paper, we will focus on conditional density estimation for massive datasets. Conditional density estimation aims to estimates the density of the response $y \in \mathcal{Y}$ given a set of predictors $(x_1, x_2, \ldots, x_p)\in \mathcal{X}$. Though, a variety of flexible models have been proposed in the last decade, density estimation remains challenging for large sample sizes and high dimensional predictors. 

The need to deal with a large number of observations motivated the literature on divide-and-conquer techniques, a class of algorithms  extensively used in density estimation, classification and prediction. Well known examples are classification and regression trees (CART) \cite{CART} and multivariate adaptive regression trees (MARS) \cite{MARS}. These algorithms fit surfaces to data by explicitly dividing the input space into a nested sequence of regions, and by fitting simple surfaces  within these regions. Though these methods are appealing to reduce the dimensionality of the problem, single tree estimates are generally associated to high variance. A possible solution to this problem would be combining estimates resulting from different trees. Well known examples are bagging \cite{Bagging}, boosting \cite{Boosting} and random forest \cite{RandomForest}. Though these algorithms can substantially reduce the variance, they can be computationally intensive. 

Mixture of experts \cite{mixtureexperts} is another divide-and-conquer algorithm particularly useful to reduce the variance associated to single tree estimates. As opposed to other divide-and-conquer algorithms, mixture of experts rely on soft partitioning algorithms that allows  observations to lie simultaneously in different subsets. A mixture of experts model is a mixture model in which the model parameters, including mixture weights, are functions of covariates. Several mixture of experts models have been proposed in the last twenty years. some of them gain flexibility by dealing with infinitely many experts \cite{infiniteMoE}  \cite{AltInfMoE}, others propose a hierarchical structure where a mixture model is fit in each subset  \cite{HierMoF} \cite{BHierMoF}.  

A significant downside of all divide-and-conquer algorithms is their poor scalability to high dimensional predictors. As the number of features increases, the problem of finding the best splitting attribute becomes intractable so that CART, MARS and multiple trees models cannot be efficiently applied. Also mixture of experts models become computationally demanding, since both mixture weights and dictionary density are predictor dependent. In an attempt to make mixture of experts more efficient sparse extensions relying on different variable selection algorithms have been proposed \cite{SparseMoF}. However, performing variable selection in high dimensions is still a challenging problem, especially when multiple parameters involved in the model, such as weights and mean functions, depend on high dimensional predictors.

 In order to efficiently deal with massive datasets, we propose a novel multiresolution approach which starts by learning a multiscale dictionary of densities, constructed as Gaussian within each set of multiscale partition tree for the features. The proposed approach is based on a two-stage algorithm where first the predictor space is recursively partitioned and then, considering the partition as fixed, a multiresolution stick-breaking process is estimated. According to the proposed process, observations can lie simultaneously in subsets located at different resolution levels. This results in a model that allows borrowing information across different resolution levels and reaches a good compromise in terms of the bias and variance trade-off. The tree partition is found by implementing a fast multiscale technique used for graph partitioning \cite{metis}.  We show that the algorithm scales efficiently to massive numbers of features, and can be implemented efficiently with gibbs sampling.  State of the art predictive performance is demonstrated for toy examples and an important application to predicting drug response to anti-depressants.

 
 
\section{Model Specification}

\subsection{ Model Structure} %In order to deal with massive datasets, efficient and scalable statistical models  need to be introduced. 
This paper will mainly focus on the problem of estimating the conditional density $f(y|x)$ of the response given a high dimensional vector of predictors, concentrated near a lower-dimensional subspace $\mathcal{M}$ embedded in $\mathcal{X}$. In dealing with massive datasets, the idea of combining different local models defined on subsets of the predictor space have increased popularity in the last decade. These methods, generally known as divide-and-conquer algorithms, aim to replace complicated conditional density functions with a combination of simple densities defined locally, on subsets of the predictor space.

Suppose we define a multiscale partition of $\mathcal{X}$.  Generation one corresponds to the entire $\mathcal{X}$ denoted as $\mathcal{X}^1$.  At generation two, $\mathcal{X}^1$ is split into two mutually exclusive partition sets, $\mathcal{X}^1=\left(\mathcal{X}^2_1, \mathcal{X}^2_2\right)$. Each subset is recursively partitioned into two subsets so that for a general partition level $\ell$ the partition will be given by $\mathcal{X}^{\ell}=\left(\mathcal{X}^{\ell}_1, \ldots, \mathcal{X}^{\ell}_{2^{\ell-1}}\right)$. Let us assume this process proceeds for $k$ levels. Let $(\ell,s)$ be the node associated to the $s$th subset at resolution level $\ell$. Let $ch(\ell,s)$ and $pa(\ell,s)$ be respectively the set of children and parents of node $(\ell, s)$. Let $A_{\ell}(x) \in \{1, \ldots, 2^{\ell-1}\}$ be the location of predictor $x$ at level $\ell$, with $A_1(x)$ equal to 1 by definition. 

We characterize the conditional density $f(y|x)$ as a convex combination of multiscale dictionary densities.  At level one, the global parent density is denoted $f_1$. The dictionary density at generation $j$ is $f_{B_j}$ with $B_j=\{j,A_j\}$, for $j=2,\ldots, k$. Then, $f(y|x)$ is defined as the convex combinations of densities $\{f_{B_j}\}_{j=1}^k$ with weights $\{\pi_{B_j(x)} \}_{j=1}^k$, i.e.
\begin{eqnarray}
f(y|x) = \sum_{j=1}^k \pi_{B_j(x)} f_{B_j(x)}(y),  \label{eq:base}
\end{eqnarray}
where $0 \le \pi_{B_j(x)} \le 1$ and $\sum_{j=1}^k \pi_{B_j(x)}=1$.

Each $B(x)$ is a set encoding the path through the partition tree up to generation $k$ specific to predictor value $x$. According to model \ref{eq:base}, one observation can simultaneously lie in subsets located at different resolution levels. This is particularly useful to reach a good compromise between the bias and variance trade-off as explained in the next section.  Moreover, it allows borrowing information across different resolution levels. Though the proposed approach reminds a mixture of experts model \cite{mixtureexperts}, the two approaches are  completely different since under (\ref{eq:base}) neither mixture weights nor dictionary densities directly depend on predictors. This allows our model to scale efficiently to high dimensional predictors.

Now let us examine the implications of model (\ref{eq:base}). For two predictor values $x$ and $x'$ located close together, it is expected that the paths will be similar, which leads to similar weights on the dictionary densities.  In the extreme case in which $x$ and $x'$ belong to the same leaf partition set, we have $B(x) = B(x')$ and the path through the tree will be the same.  Moreover, in this case, we will have $f(y|x)=f(y|x')$ so that up to $k$ levels of resolution the densities $f(y|x)$ and $f(y|x')$ are identical.  If the paths through the tree differ only in the final generation or two, the weights will typically be similar but the resulting conditional densities will not be identical. 

To derive mixture weights, a natural choice corresponds to a stick-breaking process \cite{stickbreaking}.  For each node $B_j(x_i)$ in the binary partition tree, define a stick length $V\{B_j(x_i)\} \sim \mbox{beta}(1,\alpha)$.  The parameter $\alpha$ encodes the complexity of the model, with $\alpha = 0$ corresponding to the case in which $f(y|x) = f(y)$.  We relate the weights in (\ref{eq:base}) to the stick-breaking random variables as follows: 
\begin{eqnarray*}
\pi_{B_j(x)} = V\left\{B_j(x)\right\} \prod_{B_h \in pa\left\{B_j\right\}} \left[1 - V\left\{B_h(x)\right\}\right],
\end{eqnarray*}
with $B_j(x)=\{j,A_j(x)\}$ and $V\{B_k(x)\}=1$ to ensure that $\sum_{j=1}^k \pi_{B_j}(x) = 1$.    
\vskip 12pt

\subsection{Theoretical properties} 

\section{Estimation}

The proposed approach is based on a two-stage algorithm where first the predictor space is recursively divided in different subsets using an efficient partitioning algorithm and then, considering the partition as fixed, a multiresolution stick-breaking process is estimated.  The predictor space is partitioned applying metis \cite{metis}, a fast multiscale technique used for graph partitioning. Though more complicated densities can be considered, dictionary densities $f_{B_j}$ will be estimated by assuming a normal form. In particular, densities corresponding to a particular partition set will be estimated considering observations for all subjects having predictors in that partition set. To be specific, for estimating density $f_{B_j}(y)$, we use the data $\{ y_i: x_i \in \mathcal{X}^j_{A_j} \}$. We then conduct the analysis treating the dictionary elements and partition sets as fixed and placing a prior on the weights $\pi_{B_j}$. 

Parameters involved in the dictionary density can be estimated using either frequentist or bayesian methods. Both methodologies have advantages and disadvantages. The frequentist approach, relying on maximum likelihood estimation, would allow to obtain parameter estimates in a faster and more efficient way. On the other hand, bayesian methods can avoid singularities associated with traditional maximum likelihood inference. Both methodologies will be implemented and compared for all data examples considered.

In any case, inference on stick breaking weights will be carried out using the Gibbs sampler. For this purpose, introduce the latent variable $S_i \in \{1,\ldots,k\}$, for $i=1,\ldots,n$, denoting the multiscale level used by the $i$th subject. Let $n(B_j)$ be the number of observations allocated to node $B_j$. Then, at each gibbs sampling iteration, stick breaking are sampled as follows 
\begin{enumerate}
\item Update $S_i$ by sampling from the multinomial full conditional with 
\[\mbox{Pr}( S_i = j\, |\, -) = \frac{ \pi_{B_j(x_i)}f_{B_j(x_i)}(y_i) }{ \sum_{h=1}^k \pi_{B_h(x_i)}f_{B_h(x_i)}(y_i) } \label{eq:prS}\]
\item Update stick-breaking random variable $V_{B_j(x_i)}$, for $j=1, \ldots, k$ and $i=1, \ldots, n$, from $Beta(a_p,b_p)$ with $a_p=1+n\left\{B_j(x_i)\right\}$ and $b_p=\alpha+\sum_{B_h(x_i) \in ch\{B_j(x_i)\}} n\left\{B_h(x_i)\right\}$.
\end{enumerate}

\section{Simulation Studies}
\section{Real data examples}


\newpage
\nocite{langley00}

\bibliography{multiscale3}
\bibliographystyle{icml2013}


\begin{table}[t]
\caption{Mean and standard deviations of squared error under multiscale stick-breaking (MSB), CART and random forest (RF)}
\label{table1} \vskip 0.15in \begin{center} \begin{small} \begin{sc}
\begin{tabular}{lllllllll}
\hline
\abovespace\belowspace
 $(n,p,k)$ &\multicolumn{2}{|c|}{MSB}&\multicolumn{2}{|c|}{Lasso}\\
\hline
$(30,10^4,10)$&$1.13 (1.26)$&82.29&1.21 (1.67) &146.84\\% dataFactor_n50


\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\end{document} 

